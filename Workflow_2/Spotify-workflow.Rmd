---
title: "Spotify Workflow"
output: html_document
date: "2024-01-21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Spotify and the Rolling Stone Magazine

In this sample workflow, we'll work with the Spotify api in its packaged form (spotifyr) as well as other data sources to answer the following question: 


Rolling Stone Magazine ranked their 100 greatest musical artists of all time. At the end of 2023, how has their music endured? Are there any features or characteristics that seem to explain enduring engagement?

Thinking like social scientists, we'll have to unpack a few things: 

* What does it mean to "endure" as an artist? 
* What do we mean by features/characteristics? 
* What other data sources can we use to complement what we can obtain from Spotify's API? 

To answer these questions, we'll use the API, the Rolling Stone Magazine page with its top artists, and we'll also grammy award information to complement this. This is a good example of a typical data science workflow, since we'll be scraping, interacting with a packaged API, and use pre-existing data in a csv format. We will--to combine the most data science skills in one go that we can--use a relational database. 

### Data Collection 

Before we can even think of the analyis, let's think of all the data that we'll need to answer the question. We probably want to scrape the list of artists from the magazine, and then match these up with their Spotify ID's so that we can interact with the API. Make sure to look at the [documentation](https://developer.spotify.com/documentation/web-api) of the API carefully, so that we don't abuse our rights, but also so that we don't scrape useless information. 

In order to interact with the API you'll need an account, so if you want to replicate this analysis on your own you should do that and save your log in details to a .env file. However, since I'll make the relational database available, you can also download that and run it on your machine! Without further adue, let's scrape the content from Rolling Stone Magazine. 

I like to load my packages all in one place so I can spot mistakes more easily: 
```{r, eval = TRUE, message = FALSE, warning=FALSE}
# Packages for the project 
library(xml2)
library(tidyverse)
library(dplyr)
library(RSelenium)
library(rvest)
library(spotifyr)
library(DBI)
library(RSQLite)
library(ggplot2)
library(corrplot)
library(RColorBrewer)
```

```{r, eval=FALSE}
# Scraper function for the Rollingstone magazine
scrape_artist_ranking <- function(url) {
  # Start the Selenium driver
  rD <- rsDriver(browser = "firefox", verbose = FALSE, port = netstat::free_port(random = TRUE), chromever = NULL)
  driver <- rD[["client"]]
  
  # Initialize an empty data frame with column names
  artist_ranking <- data.frame(Rank = integer(), Artist = character(), stringsAsFactors = FALSE)
  
  # Navigate to the selected URL address
  driver$navigate(url)
  Sys.sleep(2)
  
  # Close cookies consent
  close_cookies <- driver$findElement(using = "xpath", value = '//*[@id="onetrust-accept-btn-handler"]')
  close_cookies$clickElement()
  
  # Loop over the entire range (1 to 100)
  for (i in 1:100) {
    # Determine the XPath based on the position
    # "load more" button at the bottom of the page means xpaths change even if it's infinite scroll
    xpath <- if (i <= 50) {
      paste0('//*[@id="pmc-gallery-vertical"]/div[1]/div/div[', i, ']/article/h2')
    } else {
      paste0('//*[@id="pmc-gallery-vertical"]/div[2]/div/div[', i - 50, ']/article/h2')
    }
    
    # Find artist element using the constructed XPath
    artist_element <- driver$findElement(using = "xpath", value = xpath)
    
    # Get the text of the artist element
    artist_name <- artist_element$getElementText()
    
    # Add a row to the result data frame
    artist_ranking <- rbind(artist_ranking, c(i, artist_name))
    
    # Click "Load More" button after the first 50 iterations
    if (i == 50) {
      load_more <- driver$findElement(using = "xpath", value = '//*[@id="pmc-gallery-vertical"]/div[2]/a')
      load_more$clickElement()
      # Wait for the new page to load
      Sys.sleep(2)
    }
  }
  
  # Switch the ordering of the artist names
  artist_ranking <- artist_ranking %>%
    rename(Artist = X.Talking.Heads., Rank= X1L) %>%
    mutate(Artist = ifelse(Artist == "Parliament and Funkadelic", "Parliament Funkadelic", Artist), Artist = rev(Artist))
  
  # Stop the Selenium driver
  rD[["server"]]$stop()
  
  # Return the result data frame
  return(artist_ranking)
}
```
```{r, eval = FALSE}
# Call the function with the provided URL
page_url <- "https://www.rollingstone.com/music/music-lists/100-greatest-artists-147446/"
artist_ranking <- scrape_artist_ranking(page_url)
```

We have a data table! Great, now we'll need to make sense of who these artists are in terms of Spotify. This will require use to interact with the API, where we'll input the artists' names and retrieve their IDs. Note that my path for the permission to the API is relative, you should input your own there. 

```{r, eval = FALSE}
# From the artist names in Rollingstone get the spotify IDs
# Authenticate with Spotify and obtain an access token
  # Read the Spotify API credentials from Renviron
  
readRenviron("Documents/myenvs/spotifyapi.env.R")
clientID <- Sys.getenv("SPOTIFY_CLIENT_ID")
spot_key <- Sys.getenv("SPOTIFY_CLIENT_SECRET")

# Set Spotify API credentials
Sys.setenv(SPOTIFY_CLIENT_ID = clientID)
Sys.setenv(SPOTIFY_CLIENT_SECRET = spot_key)

# Authenticate with Spotify and obtain an access token
access_token <- get_spotify_access_token()
get_spotify_ids <- function(artist_ranking) {
  
  # Extract the Artist column as a vector
  artist_names <- artist_ranking$Artist
  
  # Initialize an empty list to store the results
  result_list <- list()
  
  # Search for artists and retrieve their Spotify IDs
  for (artist in artist_names) {
    artist_data <- search_spotify(
      artist,
      type = "artist",
      authorization = access_token
    )
    
    # Extract the first result
    if (nrow(artist_data) > 0) {
      spotify_id <- artist_data$id[1]
      result_list[[artist]] <- data.frame(Artist = artist, Spotify_ID = spotify_id, stringsAsFactors = FALSE)
    } else {
      # Handle the case where no result is found
      result_list[[artist]] <- data.frame(Artist = artist, Spotify_ID = NA, stringsAsFactors = FALSE)
    }
  }
  
  # Combine the list of data frames into one data frame
  artist_ids <- do.call(rbind, result_list)
  
  # Remove row names
  rownames(artist_ids) <- NULL
  
  return(artist_ids)
}
# Call the function with the provided artist_ranking data frame
artist_ids <- get_spotify_ids(artist_ranking)
```

We have the IDs that match the names, so now we can look at artist-specific features, using the Spotify "language" in some sense. It might be appropriate to start off here with some "general" information about the artists, like what genres they play, what kind of songs are popular etc. Spotify has a really handy function for this called artist_catalogue. So let's use that. 

```{r, eval = FALSE}
# Function that will extract the artist "catalogue" from Spotify 
# will use the artist IDs to get all the other artist information 
get_artist_catalogue <- function(artist_ids) {
  # Extract the artist IDs from the previous API call table 
  artist_id_list <- artist_ids$Spotify_ID
  
  # Create an empty data.frame to store the results 
  get_artists_table <- data.frame()
  
  # Using the get_artists method from the API, which will return from the ID, 
  # the popularity, genre, and followers of the artists
  for (j in artist_id_list[1:50]) {
    artist_data <- get_artists(
      j,
      authorization = get_spotify_access_token()
    )
    
    # add to the dataframe 
    get_artists_table <- bind_rows(get_artists_table, artist_data)
  }
  
  # Need 2 for loops because the get_artists call is limited to 50 IDs 
  # Repeat the above procedure from artists pos. 51-100
  get_artists_table2 <- data.frame()
  
  for(j in artist_id_list[51:100]){
    artist_data2 <- get_artists(
      j,
      authorization = get_spotify_access_token()
    )
    
    get_artists_table2 <- bind_rows(get_artists_table2, artist_data2)
  }
  
  # combine both tables, with table2 at the "bottom" of table 1
  artist_catalogue <- rbind(get_artists_table, get_artists_table2)
  
  # select relevant columns 
  # unnest the genres column to have tidy-long data where each row is a genre-artist combination 
  artist_catalogue <- artist_catalogue %>%
    unnest(genres) %>%
    select(-href, -images, -uri, -external_urls.spotify, -followers.href)
  
  return(artist_catalogue)
}

# Call the function with the provided artist_ids data frame
artist_catalogue <- get_artist_catalogue(artist_ids)
```

Now that we have some general info, we might want to ask ourselves what the top songs of an artist are. If we're looking to find some key features that explain their success, we might want to start with some key songs, and analyze those. After all, an artist is only as popular as their top tracks. 

Spotify allows us here to input a "market". This essentially means a country of relevance. Because the US is the largest Spotify market in the world, it seems appropriate to use that to extract the top songs of a given artist. It would be redundant to run this for every single country, but if your analysis calls for it, you could customize this further and focus on a different market of interest. I'm only proceeding so far in general terms!

```{r message = FALSE, eval = FALSE}
# Getting the top tracks per artist in the US "market"
# US is the largest Spotify market
get_top_tracks_us <- function(artist_id_list) {
  # Initialize an empty dataframe
  get_top_tracks <- data.frame()
  
  # Loop for the first 50 artists
  for (i in artist_id_list[1:50]) {
    top_tracks <- get_artist_top_tracks(
      i,
      market = "US",
      authorization = get_spotify_access_token()
    )
    
    # Unnest the results
    # Make a new column for co-artists containing the number of "participants" for each track 
    top_tracks <- top_tracks %>% 
      unnest_wider(artists, names_sep = "_") %>%
      mutate(
        co_artists = map_dbl(artists_id, length),
        artists_id = map_chr(artists_id, ~ ifelse(length(.) > 0, toString(.[1]), NA_character_))
      )
    
    # Convert columns to character to ensure consistency
    top_tracks <- mutate_all(top_tracks, as.character)
    
    # Identify the ID column dynamically
    id_column <- intersect(names(top_tracks), names(get_top_tracks))
    
    # Check for unique IDs before appending to the dataframe
    unique_top_tracks <- anti_join(top_tracks, get_top_tracks, by = id_column)
    
    # Append to the dataframe
    get_top_tracks <- bind_rows(get_top_tracks, unique_top_tracks)
  }
  
  # Loop for the next 50 artists
  for (j in artist_id_list[51:100]) {
    top_tracks <- get_artist_top_tracks(
      j,
      market = "US",
      authorization = get_spotify_access_token()
    )
    
    # Unnest the results
    # Make a new column for co-artists containing the number of "participants" for each track 
    top_tracks <- top_tracks %>% 
      unnest_wider(artists, names_sep = "_") %>%
      mutate(
        co_artists = map_dbl(artists_id, length),
        artists_id = map_chr(artists_id, ~ ifelse(length(.) > 0, toString(.[1]), NA_character_))
      )
    
    # Convert columns to character to ensure consistency
    top_tracks <- mutate_all(top_tracks, as.character)
    
    # Identify the ID column dynamically
    id_column <- intersect(names(top_tracks), names(get_top_tracks))
    
    # Check for unique IDs before appending to the dataframe
    unique_top_tracks <- anti_join(top_tracks, get_top_tracks, by = id_column)
    
    # Append to the dataframe
    get_top_tracks <- bind_rows(get_top_tracks, unique_top_tracks)
  }
  
  # Convert to tibble and select relevant columns
  get_top_tracks <- get_top_tracks %>% select(
    artists_id, duration_ms, explicit, id, name, popularity, track_number, album.album_type, album.id, album.name, album.release_date, album.release_date_precision, album.total_tracks, album.type, co_artists
  )
  
  return(get_top_tracks)
}

artist_id_list <- artist_ids$Spotify_ID
# Call the function with the provided artist_id_list
top_tracks_us <- get_top_tracks_us(artist_id_list)
```

And with this, we have a handy table for all the US top tracks per artist! This is usually 10 songs, but you'll notice that for some artists there are 8 or 9. This is not important to us right now, and we can think about how to fix that later on, if relevant to our analysis. For now let's move on to collecting all the markets for which each song in this top 10 per artist is available. It could be that some songs are not available in some parts of the world, and that these artists can't be played there.

```{r, eval = FALSE}
# Function that will get the top track table with all the markets, using track ids from the US market data 
get_top_track_table_with_markets <- function() {
  # Extracting track IDs from the top_tracks_us data frame
  unique_track_ids <- unique(top_tracks_us$id)
  
  # Initialize an empty data frame to store market information
  track_markets <- data.frame()
  
  # Loop through each unique track ID
  for (i in unique_track_ids) {
    top_track_markets <- get_tracks(
      i,
      market = NULL,
      authorization = get_spotify_access_token()
    )
    
    # Calculate available markets count for both track and album
    top_track_markets_flat <- top_track_markets %>%
      mutate(available_markets_count = lengths(available_markets)) %>%
      mutate(album.available_markets_count = lengths(album.available_markets)) %>%
      distinct(id, .keep_all = TRUE)
    
    # Append to the track_markets data frame
    track_markets <- bind_rows(track_markets, top_track_markets_flat)
  }
  
  # Join the track and album market info to the top_tracks_us table
  top_track_table <- left_join(
    top_tracks_us,
    track_markets %>%
      select(id, available_markets_count, album.available_markets_count),
    by = "id"
  )
  
  return(top_track_table)
}

# Call the function
top_track_table <- get_top_track_table_with_markets()
```

Now that we have all of this information, which I've manipulated in the code to be displayed as a count of available markets, rather than a list of all the countries where the song can be played, we can join this table with the previous one. This makes sense to do since they both pertain to general information about the top tracks. When making relational databases, we don't want to juggle unnecessarily between tables with a bunch of JOIN statements. We want to divide the information in relevant sub-sections that are concise, but complete. 

```{r, eval=FALSE}
# joining the track and album market info to the tracks table 
top_track_table_joined <- left_join(top_tracks_us, top_track_table %>%
                      select(id, available_markets_count, album.available_markets_count),
                    by = "id")
```

Now that we have all of this general information about the top songs, we might want to look at the key audio features of these songs. This is likely to be the thing that has the most effect on popularity, or at the very least _some_ effect. We have to be careful here, since we now have nearly 1000 songs of interest, but the API asks us to limit our query to 100 songs. This means essentially 10 artists at a time. For this, we'll divide the data into batches, so that we don't over-query. 
```{r, eval = FALSE}
# Using the get_track_audio_features which takes max 100 song IDs to get info on all top songs 
# Looping over batches of data to match query requirement 

# song IDs directly from pre-existing table 
song_ids <- top_track_table_joined$id

# Function to retrieve the audio features 
get_audio_feat <- function(song_ids) {
  # initialize data frame
  tracks_audio <- data.frame()

  # determine the number of batches based on the API limit
  batch_size <- 100
  num_batches <- ceiling(length(song_ids) / batch_size)

  # loop over batches
  for (batch in 1:num_batches) {
    # get start and end indices for the current batch
    start_index <- (batch - 1) * batch_size + 1
    end_index <- min(batch * batch_size, length(song_ids))

    # extract song IDs for the current batch
    current_batch_ids <- song_ids[start_index:end_index]

    # loop over song IDs in the current batch to get the features
    for (j in current_batch_ids) {
      tracks_audio_feat <- get_track_audio_features(
        j,
        authorization = get_spotify_access_token()
      )
      # append to dataframe
      tracks_audio <- bind_rows(tracks_audio, tracks_audio_feat)
    }
  }

  return(tracks_audio)
}

# Call the function with your song IDs
tracks_audio <- get_audio_feat(song_ids)

# clean output to contain only relevant elements before uploading to database
tracks_audio <- tracks_audio %>%
  select(-analysis_url, -track_href, -uri, -type)
```

Great! We have a _lot_ of information from the API now. We can move on to our last bit of data collection: the csv file. This file was pre-compiled, presumably from someone who scraped the Grammy Award website, so we can use it as it is. We could, of course, scrape the website ourselves. However, if data already exists, it's good practice to use that, rather than querying the server for scraping purposes, especially because every transaction is costly!

Here I just read the file that's in the repository and do some basic artist name cleaning for coherence. 

```{r warning = FALSE, message = FALSE, eval = FALSE}
# data source: https://www.kaggle.com/datasets/unanimad/grammy-awards/versions/2?resource=download
gram <- read.csv("the_grammy_awards.csv")

# from gram selecting only the rows that pertain to the artists in Rollingstone magazine
artist_names <- artist_ids$Artist

# establish regex pattern to capture the different variations of the artist's name
regex_patterns <- paste0("\\b", artist_names, "\\b(?!(\\s*:|\\s+Anthology))", collapse = "|")

# Filter rows based on the regex patterns
# Fill empty values in the "workers" column with the corresponding values from the "nominee" column
grammys_filtered <- gram %>%
  mutate(workers = ifelse(workers == "", nominee, workers)) %>%
  filter(str_detect(workers, regex_patterns)) %>%
  mutate(win_or_nomination = ifelse(winner, "Win", "Nomination"))

# Summary table saved as final table to count the number of wins and nominations for each artist that is in the list
summary_grammys <- grammys_filtered %>%
  mutate(artist = str_extract(workers, regex_patterns)) %>%
  group_by(artist, win_or_nomination) %>%
  summarize(count = sum(n())) %>%
  pivot_wider(names_from = win_or_nomination, values_from = count, values_fill = 0)
```

Congratulations! We've collected all our data! Now we can write all of these tables to a handy database, and go from there. 


```{r, eval = FALSE}
# create a new database and write all the tables to the database separately
spotify_db <- DBI::dbConnect(RSQLite::SQLite(), "spotify_db.db")

#checking that the database created exists 
file.exists("spotify_db.db")

# write the rankings table 
dbWriteTable(spotify_db, "artist_rankings", artist_ranking)

# write the artist_ids table 
dbWriteTable(spotify_db, "artist_ids", artist_ids)

# write the artist_catalogue table containing info about artists 
dbWriteTable(spotify_db, "artist_stats", artist_catalogue)

# write the top tracks information table per artist
dbWriteTable(spotify_db, "top_tracks_stats", top_track_table_joined)

# write the audio features per (top) track
dbWriteTable(spotify_db, "top_track_audio_feat", tracks_audio)

# Write the grammys data set 
dbWriteTable(spotify_db, "grammy_stats", summary_grammys)
```


### Data Analysis

We've gathered data, now we can analyze it! Considering all the of the data that we have available, we can ask ourselves a few guiding questions, which will walk us through the analysis part. I note here that I use the word "analysis" very loosely, and that I only walk through data visualizations, that can be the _starting point_ of an analysis. These are not to be taken with statistical precision, or to show anything for fact. 

Let us consider these questions before getting into our **VIZ**

* How many artists are still truly popular today? And what measure from the data can we use? 
* Is there any other way to understand popularity? 
* Does an artist's genre affect popularity? 
* Does market availability help popularity? 
* Are there any audio features that are mostly reflected in popular songs from popular artists? 
* Do popular artists win more awards? Or are they more popular because they win more awards? Is there a relationship there? 

Let's walk through these questions with some helpful plots. 

First, what do we mean by truly popular? Let's look at the distribution of popularity scores according to Spotify (you can read more about how these are calculated on the API documentation) amongst our 100 best artists of all time. 

```{r, eval = TRUE, warning = FALSE}
# connect to db
spotdb <- dbConnect(RSQLite::SQLite(), "spotify_db.db")

# Get the artist popularity from SQL query 
unique_pop <- dbGetQuery(spotdb, "SELECT DISTINCT(id), popularity
           FROM artist_stats")
# Plot distribution with density
ggplot(unique_pop, aes(x = popularity)) +
  geom_density(binwidth = 5, fill = "blue", alpha = 0.7) +
  labs(title = "Figure 1: Popularity Distribution of Artists", x = "Popularity", y = "Frequency") + 
  theme_minimal()
```

The data being slightly negatively skewed, we can to divide the popularity measures into quartiles (very low popularity, low popularity, popular, very popular) to distinguish popularity levels within an already popular group. This might help us increase the granularity of our data! Let's update the database with this new information. 

```{r, eval = FALSE}
# divide the popularity variable into quartiles with custom names
unique_pop$popularity_quartile <- cut(unique_pop$popularity, 
                                      breaks = quantile(unique_pop$popularity, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE), 
                                      labels = c("Very Low", "Low", "Popular", "Very Popular"))

# Create a temporary table with the new data
dbWriteTable(spotdb, "temp_artist_stats", unique_pop, append = TRUE,)

# Update the existing table with the new column
dbExecute(spotdb,"UPDATE artist_stats
      SET popularity_quartile = (
        SELECT popularity_quartile
        FROM temp_artist_stats
        WHERE artist_stats.id = temp_artist_stats.id
      )" 
      )

# Drop the temporary table
dbExecute(spotdb, "DROP TABLE IF EXISTS temp_artist_stats")

# connect to db
db_connection <- dbConnect(RSQLite::SQLite(), "spotify_db.db")

# Check that it worked: 
check_table(db_connection, table_check_3)
```

```{r,eval=TRUE}
# connect to database
spotdb <- dbConnect(RSQLite::SQLite(), "spotify_db.db")

# Query to select name, id, popularity and quartile of artists
endure1 <- dbGetQuery(spotdb, "SELECT DISTINCT(id), name, popularity, popularity_quartile
           FROM artist_stats
           WHERE popularity_quartile LIKE 'Very Popular' OR popularity_quartile LIKE 'Popular'
           ORDER BY popularity DESC")

# print nicely the number of artists in the top 2 quartiles
cat("The number of artists who fall into the Very Popular or Popular quartiles is",length(endure1$name))
```

With this new division of the data, 50 artists fall in (very) popular categories, half of the "best artists of all time" endured through 2023 under this definition. Since we divided the data into quartiles, and we observed earlier that the data was slightly negatively skewed, the fact that exactly 50% of our data is scoring highly, means that there are outliers affecting the visual distribution of the data. This measure reflects entire discographies, so we can repeat the process for artists’ top 10 tracks, limiting the downward effect of unpopular/remastered tracks.

```{r eval=TRUE}
# Query to get the popularity of the tracks only 
distr_tracks <-dbGetQuery(spotdb, "SELECT popularity, artists_id
                          FROM top_tracks_stats")

# Convert to numeric to ensure all is correctly formatted
distr_tracks$popularity <- as.numeric(distr_tracks$popularity)
# Create a histogram
ggplot(distr_tracks, aes(x = popularity)) +
  geom_histogram(fill = "lightblue", color = "grey", alpha = 0.7, bins = 50) +
  labs(title = "Figure 2: Track Popularity Distribution", x = "Track Popularity Score", y = "Count") +
  theme_minimal()
```

As Figure 2 illustrates, most scores are on the right-hand-side of the mean; top tracks are unsurprisingly quite popular among Spotify users. I make relative measures of these track popularity scores using split as I did earlier and update the database.

```{r, eval = FALSE}
# divide the popularity for tracks into quartiles with custom names
distr_tracks$popularity_quartile <- cut(distr_tracks$popularity, 
                                      breaks = quantile(distr_tracks$popularity, probs = c(0, 0.25, 0.5, 0.75, 1), na.rm = TRUE), 
                                      labels = c("Very Low", "Low", "Popular", "Very Popular"))

# Create a temporary table with the new data
dbWriteTable(spotdb, "temp_track_stats", distr_tracks, append = TRUE, row.names = FALSE)

# Check if the column exists
col_exists <- dbGetQuery(spotdb, 'PRAGMA table_info("top_tracks_stats")')$name %in% "popularity_quartile"

# If the column doesn't exist, add it
if (!any(col_exists)) {
  dbExecute(spotdb, 'ALTER TABLE top_tracks_stats ADD COLUMN popularity_quartile TEXT;')
}

# Update the existing table with the new column
dbExecute(spotdb, '
  UPDATE top_tracks_stats
  SET popularity_quartile = temp_track_stats.popularity_quartile
  FROM temp_track_stats
  WHERE top_tracks_stats.id = temp_track_stats.artists_id;
')


# Drop the temporary table
dbExecute(spotdb, "DROP TABLE IF EXISTS temp_track_stats")

db_connection <- dbConnect(RSQLite::SQLite(), "spotify_db.db")
# Check that it worked: 
check_table(db_connection, table_check_4)
```

```{r eval=TRUE}
# connect to DB and look at what proportion of songs per artist have high popularity or are popular (top 50%)
spotdb <- dbConnect(RSQLite::SQLite(), "spotify_db.db")

pop_tracks <- dbGetQuery(spotdb, " SELECT 
                      DISTINCT track.name, 
                      art.name, 
                      track.artists_id, 
                      track.popularity, 
                      track.popularity_quartile
                      FROM top_tracks_stats AS track
                      LEFT JOIN artist_stats AS art ON art.id = track.artists_id
                      WHERE track.popularity_quartile LIKE 'Very Popular' OR track.popularity_quartile LIKE 'Popular' AND art.name IS NOT NULL
                      ORDER BY track.popularity DESC")
# Print nicely the number of artists
cat("The number of artists in the list having at least one (very) popular song is:", length(unique(pop_tracks$artists_id)))
```

With this new information, 85% of artists have at least one (very) popular song, a sign of relative endurance in the group. However, numerous popular songs are duplicates, remixes, covers or remasters, suggesting low diversity—an important selection criterion in Rolling-Stone. Considering this, overall artist popularity is a more suitable measure. Consequently, about 50% of artists in the list can be considered to have truly endured up to the standard of the "best artists of all time."

We asked ourselves earlier whether the number of genres played by an artist have an effect on popularity. Does more genres mean more people to pick from in terms of audience? Let's find out. 

```{r, eval=TRUE, fig.width=10, fig.height=8, warning = FALSE}
# Query to select the number of genres, popularity, name and quartiles of all artists
genre_pop1 <- dbGetQuery(spotdb, "SELECT COUNT(genres) AS genres_count, popularity, name, popularity_quartile
FROM artist_stats
GROUP BY name")

# ensuring any NAs in the quartiles go into the very low range
genre_pop1 <- genre_pop1 %>%
  mutate(popularity_quartile = ifelse(is.na(popularity_quartile), "Very Low", as.character(popularity_quartile)))

# Plot the genre number against popularity, grouping by popularity quartiles per artist 
ggplot(genre_pop1, aes(x = genres_count, y = popularity, color = popularity_quartile)) +
  geom_point(aes(size = genres_count), alpha = 0.4, position = position_jitter(width = 0.1, height = 0.2)) +
  scale_size_continuous(range = c(3, 15)) +  # Keep size legend
  geom_smooth(method = "lm", se = FALSE, color = "darkgrey", linetype = "dotted", alpha = 0.7, show.legend = FALSE) +  
  coord_cartesian(ylim = c(30, 100), xlim = c(0, 10)) +
  labs(title = "Figure 3: Bubble Chart of Genres and Popularity per Artist",
       x = "Number of Genres",
       y = "Popularity",
       caption = "Bubble size = Number of Genres") +
  theme_minimal() +
  scale_color_manual(values = c("Very Popular" = "purple", 
                                "Very Low" = "deepskyblue2", 
                                "Popular" = "darkorange", 
                                "Low" = "hotpink")) +
  guides(color = guide_legend(title = "Quartile Legend"), size = FALSE)
```

Figure 3 shows that artists playing more genres actually tend to be less popular! This is especially evident in the top popularity quartile, which generally has fewer genres. A clear division exists between popularity quartiles and genre counts. High endurance is not attributable to high genre diversity, but lower diversity may contribute to high endurance. Again, since we're not running any statistical tests, we can only make visual conjectures. 


We also thought about how different markets might affect popularity. Perhaps more exposure in markets allows artists to boom more. Let's have a look in Figure 4: 

```{r, eval=TRUE, warning = FALSE}
ind_av_mrkt_count1 <- dbGetQuery(spotdb, "SELECT ROUND(AVG(t.available_markets_count), 1) AS Average_Market_Count, t.artists_id, a.name, a.popularity_quartile, a.popularity
           FROM top_tracks_stats as t 
           LEFT JOIN artist_stats AS a ON a.id = t.artists_id
           GROUP BY t.artists_id")

ind_av_mrkt_count1 <- ind_av_mrkt_count1 %>%
  mutate(popularity_quartile = ifelse(is.na(popularity_quartile), "Very Low", as.character(popularity_quartile)))

ggplot(ind_av_mrkt_count1, aes(x = Average_Market_Count, y = popularity)) + 
  geom_point(aes(color = popularity_quartile)) + 
  geom_smooth(method = "lm", se = TRUE, linetype = "dotted", color = "grey", fill = "bisque2", show.legend = FALSE) + 
  labs(title = "Figure 4: Artist Popularity and Average Market Availability",
       x = "Top Tracks' Market Availability", y = "Artist Popularity") + 
  theme_minimal() +
  facet_wrap(~popularity_quartile, scales = "free")
cat("Mean market access across all artists:", round(mean(ind_av_mrkt_count1$Average_Market_Count), 2))
cat("\nRange of market access across all artists:", range(ind_av_mrkt_count1$Average_Market_Count))
```

In Figure 4, top track availability spans 1 to 184 (Spotify's maximum markets). The mean, 163.85, indicates widespread global availability, likely contributing to sustained popularity in 2023. While increased market access benefits very low-popularity artists, the trend reverses for very high and low-popularity artists. This suggests that, for the most enduring artists, market availability doesn't significantly contribute to endurance, likely due to their established global presence. For lesser-known artists, however, it may present an opportunity. It can thus not explain the endurance of the top 50 artists. 

So far, it doesn't look like any our of original hypothesized causes of popularity have an effect. Perhaps audio features will have an effect on an artist's popularity. After all, perhaps people enjoy common sounds and tracks because of some common underlying features!

```{r, eval=TRUE}
audio_feat_combined <- dbGetQuery(spotdb, "SELECT 
f.danceability AS Danceability, 
                    f.energy AS Energy, 
                    f.tempo AS Tempo, 
                    f.duration_ms AS Duration_ms, 
                    f.valence AS Positivity, 
                    f.loudness AS Loudness, 
                    f.speechiness AS Speechiness, 
                    f.acousticness AS Acousticness,
                    f.instrumentalness AS Instrumentalness,
                    a.popularity AS Artist_pop,
                    t.co_artists AS Co_artists, 
                    t.popularity AS Track_pop 
FROM top_track_audio_feat AS f
           LEFT JOIN top_tracks_stats AS t
                  ON t.id = f.id
           LEFT JOIN artist_stats AS a 
                  ON t.artists_id = a.id
          WHERE a.popularity_quartile LIKE 'Very Popular' OR a.popularity_quartile LIKE 'Popular'
          GROUP BY t.id
")
# ensure all variables are numeric before using the correlation matrix
audio_feat_combined$Co_artists <- as.numeric(audio_feat_combined$Co_artists)
audio_feat_combined$Track_pop <- as.numeric(audio_feat_combined$Track_pop)

# correlation matrix for all variables in the table for all artists' whose popularity is "popular" and "very popular"
# ie. top 25 artists
correlation_matrix <- cor(audio_feat_combined)
# corrplot of variables, each one with all the others
corrplot(correlation_matrix, 
         method = "color", 
         type = "upper", 
         order = "hclust",
         main = "Figure 5: Correlation Matrix of Top 25 Artists' Top \n Tracks' Audio Features and Popularity",
         tl.col = "black", 
         tl.srt = 45, 
         cl.cex = 0.7,
         col = brewer.pal(n = 8, name = "PRGn"), 
         mar = c(1, 1, 3.5, 2), 
         tl.cex = 0.8,
         cl.align.text = "l", 
         cl.offset = 0.3
) 
```

Figure 5 reveals significant correlations only between energy and acousticness, and energy and loudness among the top 50 artists' tracks. Despite having expected endurance-inducive characteristics to manifest in top tracks, no significant correlation exists between popularity and any other variable. I don't know about you, but I'm starting to believe Rolling Stone Magazing... perhaps the best music just can't be explained. Maybe we just love it because it makes us feel things we can't explain. 

We do have one last piece of information we haven't looked at yet: Grammy Awards. A potential key to understanding an artist's endurance lies in their reputation. Acknowledging the cyclical nature of reputation and popularity, we can think of it in terms of reputation-validation--sustaining endurance through awards, prizes, and positive commentary. To explore this, let's examine Grammy Award data.

```{r, eval=TRUE, fig.width=12, fig.height=8}
# SQL query to combine artist names with popularity quartile to extract only the grammy data 
# Not all artists have Grammy data
nominations_grammys <- dbGetQuery(spotdb, "SELECT g.artist, 
            g.win, 
            g.nomination, 
            a.popularity_quartile
            FROM grammy_stats AS g
           LEFT JOIN artist_stats AS a 
           ON a.name = g.artist
           GROUP BY g.artist")

# turn the output into long format to be able to position bars next to each other on the same pane
nominations_long <- nominations_grammys %>%
  gather(key = "variable", value = "value", Win, Nomination)

# Remove missing values from analysis 
nominations_long <- na.omit(nominations_long)

# Ensure popularity_quartile is a factor with proper ordering
nominations_long$popularity_quartile <- factor(
  nominations_long$popularity_quartile,
  levels = c("Very Popular", "Popular", "Low", "Very Low")  # Adjust these levels based on your data
)

# Plot artists nominations and wins for grammys between 1958 and 2019
# Some artists are more "recent" and will have naturally gathered less prizes but the trends are useful 
ggplot(nominations_long, aes(x = factor(artist), y = value, fill = variable)) +
  geom_bar(position = "dodge", stat = "identity", alpha = 0.8) +
  scale_fill_manual(values = c("Win" = "lightpink", "Nomination" = "purple"), 
                    labels = c("Nominations with no Win", "Nomination with win")) +
  scale_y_continuous(minor_breaks = seq(0, max(nominations_long$value), by = 1)) +
  theme(axis.text.x = element_text(angle = 75, hjust = 1)) +
  labs(fill = "Legend", title = "Figure 6: Grammy Award Wins and Nominations per Artist", y = "Frequency", x = "Artists") +
  facet_wrap(~popularity_quartile, scales = "free_x")
```

Figure 6 indicates that popular/very popular artists exhibit high levels of wins and nominations, suggesting significant external validation compared to their low/very low popularity counterparts. While the direction of the relationship with endurance remains unclear, a distinct disparity between groups exists, and external validation emerges as a potential factor influencing endurance, though it could be the product of it.

I hope this workflow gave you some inspiration on how to manage your data (and hopefully how to make pretty plots), and also showed you that while pretty graphs are awesome, if we want to answer real world questions, we might need some causal inference.

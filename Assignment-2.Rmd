---
title: "Assignment-2"
output: html_document
date: "2023-11-21"
---

```{r setup, include=FALSE, eval = TRUE, echo = FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(eval = FALSE)
```

## Exercise 1
I start here by creating a new database to be used for the entire assignment, and check that it exists. 

```{r, eval = TRUE, echo = TRUE}
#creating a new database called "assignment2_database.db"
assignment2_db <- DBI::dbConnect(RSQLite::SQLite(), "assignment2_database.db")
```
```{r, echo = TRUE}
#checking that the database created exists 
file.exists("assignment2_database.db")
```

## Exercise 2

### Part A 

```{r, eval = TRUE, message = FALSE, warning=FALSE}
library(RSelenium)
library(tidyverse)
library(rvest)
library(xml2)
library(netstat)
library(RSQLite)
library(httr)
library(tidycensus)
library(tigris)
library(ggmap)
library(DBI)
library(sf)
library(sp)
library(tmap)
```
For this exercise, I decide to use the selector for all the tables in the wikipedia page, but then write my function with a for loop that will consider only the first two. This is more efficient than fetching the individual selctors of the different tables, and effectively scraping everything twice. I also skip the header, as I make my own in the tibble I create. 

The difficulty of this task is not to extract the information of each university, but rather their individual URL which is absolute, rather than relative to the navigation on the wikipedia page. For this, I paste the relative URL to that of the original wikipedia page. 

I do not spot any missing data at this stage, so I do not take any additional measures here. 
```{r}
uni_scraper <- function() {
  
  # Specify the URL of the Wikipedia page
  uni_url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
  # Read the HTML content from the specified URL
  page <- read_html(uni_url)
  
  # Extract all tables containing information about research universities
  universities_tables <- html_elements(page, css = "table.wikitable")
  
  # Initialize empty vectors to store the data
  names <- c()
  status <- c()
  city <- c()
  state <- c()
  urls <- c()
  
  # Loop through each table (but limit to tables 1-2 since the others are not R1 or R2 research centers)
  for (table_index in seq_along(universities_tables[1:2])) {
    
    universities_table <- universities_tables[[table_index]]
    
    # Extract the rows from the table
    rows <- html_elements(universities_table, "tr")
    
    # Loop through each row starting from the second row (skipping header)
    for (i in 2:length(rows)) {
      row <- rows[i]
      
      # Extract columns from the row
      columns <- html_elements(row, "td")
      
      # Extract data and append to the respective vectors
      names <- c(names, html_text(columns[1]))
      status <- c(status, html_text(columns[2]))
      city <- c(city, html_text(columns[3]))
      state <- c(state, html_text(columns[4]))
      
      # Extract the URL from the first column
      url_element <- html_element(row, "a") %>% html_attr("href")
      full_url <- ifelse(is.na(url_element), NA, paste("https://en.wikipedia.org",  url_element, sep = ""))
      urls <- c(urls, full_url)
    }
  }
  
  # Create a data frame with the collected data
  universities_data <- tibble(
    Name = names,
    Status = status,
    City = city,
    State = state,
    URL = urls)
  
  return(universities_data)
}
```
```{r }
# Call the scraping function
R1R2_table <- uni_scraper()
```

### Part B

I now create a new function (as opposed to adding on to the previous) as it is more readable, and will allow me to handle missing data on a smaller dataset than if I were to scrape it all at once. For this reason, I consider the same tables as earlier, with the same method, and navigate to the individual university pages. 

The coordinates are in the same position on every page, so I use the CSS .geo-dms to scrape both the latitude and logitude at once. This minimizes the processing of the data inside the function. Second, I look for endowment information in the infobox of the university's page, and scrape it while also removing any information that isn't the USD amount (eg. $180 billion), as some contain the dates of when this information was collected. Given this additional information is not available for the majority of data points, and that when it is it shows discrepancies in the year, I entirely remove it. This is one of the limitations of the data. 

Third, I choose the variable "students" for the total number of students. An alternative would have been to select the undergraduate and graduate students and sum them, but the dates for both of these student groups don't always match up, while total number of students is a measure from a single point in time. It is also noteworthy that some universities have multiple campuses, for all of which the total students are listed. In those cases, I address the issue in the function sum them. While the geographical coordinates are that of the main campus, students can be "remote" and still be part of the student body. I consider it more important to include all the students which the university has to account for, rather than what campus accounts for most of them in this case. 

```{r}
scrape_university_extra <- function() {
  # Read the HTML content from the specified URL
  url <- "https://en.wikipedia.org/wiki/List_of_research_universities_in_the_United_States"
  page <- read_html(url)
  
  # Extract all tables containing information about universities
  universities_tables <- html_elements(page, css = "table.wikitable")
  
  # Initialize empty vectors to store the data
  names <- c()
  total_students <- c()
  endowment <- c()
  coordinates <- c()
  
  # Loop through the first two tables
  for (table_index in seq_along(universities_tables[1:2])) {
    universities_table <- universities_tables[[table_index]]
    
    # Extract the rows from the table
    rows <- html_elements(universities_table, "tr")
    
    # Loop through each row starting from the second row (skipping header)
    for (i in 2:length(rows)) {
      row <- rows[i]
      
      # Extract columns from the row
      columns <- html_elements(row, "td")
      
      # Extract university name and URL
      name <- html_text(columns[1])
      url_element <- html_element(columns[1], "a")
      university_url <- if (!is.null(url_element)) url_element %>% html_attr("href") else NA
      full_url <- ifelse(is.na(university_url), NA, paste("https://en.wikipedia.org", university_url, sep = ""))
      
      # Navigate to the university's dedicated page
      university_page <- read_html(full_url)
      
      #scrape the coordinates (lat and long. simultaneously)
      coordinates_value <- university_page %>% 
              html_element(css =".geo-dms") %>% 
              html_text()
      
      #scrape the endowment value but keep only the USD value
      endowment_value <- university_page %>% 
              html_element(css = "table.infobox th:contains('Endowment') + td") %>%
              html_text() %>%
              gsub("\\s*[\\(\\[].*", "", .)
      
      #scrape total number of students, but remove unneccesary information (different campuses etc.)
      total_students_value <- university_page %>% 
        html_element(css= "table.infobox th:contains('Students') + td") %>% 
        html_text() %>%
        str_replace_all("\\[[^\\]]*\\]|\\([^\\)]*\\)|[a-zA-Z]", " ")
      
      total_students_value <- gsub(",", "", total_students_value)
      
      if(!is.na(total_students_value) && any(str_detect(total_students_value, "\\s+"))){
        individual_numbers <- as.numeric(strsplit(total_students_value, "\\s+")[[1]])
        total_students_value <- sum(individual_numbers)
      }else{
        total_students_value <- as.numeric(total_students_value)
      }
      
      #Append the data to the vectors
      names <- c(names, name)
      coordinates <- c(coordinates, coordinates_value)
      endowment <- c(endowment, endowment_value)
      total_students <- c(total_students, total_students_value)
    }
  }
  
  # Create a tibble with the collected data using tidyverse
  universities_data <- tibble(
    Name = names,
    Coordinates = coordinates,
    Endowment = endowment,
    TotalStudents = total_students)
  
  return(universities_data)
}

```
```{r}
# Call the scraping function
university_info_extra <- scrape_university_extra()
```

```{r}
# merge the two table outputs from the two functions into one
merged_uni_table <- left_join(university_info_extra, R1R2_table, by = "Name")
```

### Part C

I now create a new variable in my original table, which will take for value "Yes" if a university is in the ivy table, and "No" if it isn't. Given that the names in the ivy table are shortened, I match partial names with the status "private (non-profit) across both tables. This ensures that partial matches, such as "Pennsylvania State" does not get confused with "University of Pennsylvania" with inputs like "penn". 

In doing so, I note that Teachers College at Columbia University is a partial match with Columbia University and is of private status. However, I manually exclude it, as it is part of Columbia University, and its students are included in the main table under Columbia University. For the sake of consistency, I thus manually prevent it from being classified as an ivy in the table. 

I then proceed onto adding the full names (as seen in the main table) to the ivy table, so that merging will be easier. I concatenate the state and county into a single variable separated by a comma to be included in the main table. For all the non-ivy universities, I leave just the state in this variable, so that comparison inside the table can be done in a single column if necessary, but I preserve the original state-only variable in my table as well. The final table does not contain a county-only column, as these would be only for ivy universities and NULL for all other universities. If a county analysis is required, it can be done directly by sub-setting the ivys in the county_state variable.  

```{r}
ivys <- read.csv("ivyleague.csv")
# if a university matches the name partially in the large table and its private, then allocate 
# "yes" to ivy status in a new column of the large table
# I manually exclude Teachers College at Columbia University for simplicity  
merged_uni_table$IsIvy <- ifelse(
  grepl(paste0(".*?(", paste(ivys$uni_name, collapse = "|"), ").*"), 
        merged_uni_table$Name, ignore.case = TRUE) &
    !grepl("Teachers College at Columbia University", merged_uni_table$Name, ignore.case = TRUE) &
    merged_uni_table$Status == "Private (non-profit)",
  "Yes",
  "No")

```
```{r}
#now adding the full names to the ivys table so that they can be merged and add information 
#about the counties and the ein
ivys$Name <- sapply(ivys$uni_name, function(uni) {
  matching_names <- merged_uni_table$Name[grepl(uni, merged_uni_table$Name, ignore.case = TRUE) & 
                                            merged_uni_table$Status == "Private (non-profit)"]
  
  if (length(matching_names) > 0) {
    return(matching_names[1])
  } else {
    return(NA)
  }
})

merged_uni_table <- left_join(merged_uni_table, ivys[, c("Name", "ein", "county")], by = c("Name" = "Name"))

#rename columns 
merged_uni_table <- merged_uni_table %>%
  rename(EIN_ivys = ein, County_ivys = county)

# Drop unnecessary columns from ivys data frame
ivys <- ivys[, !(names(ivys) %in% c("ein", "county"))]

#concatenating the county and state for all variables in merged_uni_table, where county won't show if info not available
#this will allow the table to be easily updated if county information is provided about any other university 
merged_uni_table <- merged_uni_table %>%
  mutate(county_state = ifelse(
    is.na(County_ivys),
    State,
    paste(County_ivys, State, sep = ", ") 
  ))%>%
  select(-County_ivys)
```

### Part D

Here, I write the final table, containing the ivy-league information to my database as R1R2_uni_list, for it refers to all the R1 and R2 variables. 

My function check_table takes for argument the database name and the table name and returns the dimensions of the table. It takes a query of COUNT for the number of rows, and a query of PRAGMA table_info and returns the number of columns, rows and the column names. I count the columns starting from 1 (as opposed to default 0). 

The output indicates the number of rows for each field (column) in the table in the database. If the table was not written in the database, it returns an error message. The function assumes that an attempt at writing the table to the database was made, and that one is connected to said database. 

```{r}
#writing table to database
dbWriteTable(assignment2_db, "R1R2_uni_list", merged_uni_table)
```
```{r eval = TRUE, echo = TRUE}
check_table <- function(db_name, table_name) {
  # Query to count the number of rows
  query_rows <- paste("SELECT COUNT(*) AS Number_rows FROM", table_name)
  result_rows <- dbGetQuery(db_name, query_rows)

  # Query to count the number of columns
  query_columns <- paste("PRAGMA table_info(", table_name, ")")
  result_columns <- dbGetQuery(db_name, query_columns)

  # Extract relevant information
  num_rows <- result_rows$Number_rows
  table_info <- data.frame(
    ColumnNumber = result_columns$cid + 1,  # Add 1 to start numbering from 1
    ColumnName = result_columns$name, 
    NumberRows = num_rows
  )

  return(table_info)
  dbDisconnect(db_name)
}

```
```{r, eval = TRUE, echo = TRUE}
# Checking that the table exists
check_table(assignment2_db, "R1R2_uni_list")
```


## Exercise 3

In order to scrape the rankings of ivy universities, I create an automated scraper that navigates to the rankings page of the ARWU website and then for each ivy university, scrapes the values for each year (2003, 2013, 2023). It does so sequentially with nested for loops. 

I use here a modulable x-path to select the date of interest inside the for loop with the x-path of the date element and the paste0 command. This allows the scraper to select the years 2003, 2013, and 2023 sequentially without having to re-enter their individual x-paths each time. 

I choose here to handle missingness or any other data formatting issues outside the function, so as to close the browser as fast as possible. 

```{r}
scrapeIvyRankings <- function() {
  list_ivy <- c("Harvard University", "Princeton University", "Yale University", 
                "Columbia University", "University of Pennsylvania", "Brown University", 
                "Dartmouth College", "Cornell University")

  Ivy_rankings <- data.frame(
    University = character(),
    Ranking = character()
  )
  rank_url <- "https://www.shanghairanking.com/"
  # Start the Selenium server:
  rD <- rsDriver(browser=c("firefox"), verbose = F, port = netstat::free_port(random = TRUE), chromever = NULL) 
  driver <- rD[["client"]] # note this alternative but equivalent call for setting the driver client

  # Navigate to the selected URL address
  driver$navigate(rank_url)
  
  # This step only need to happen once, as we can then easily just navigate to the date 

  ranking_page <- driver$findElement(using = "xpath", value = '//*[@id="arwu"]/div[1]/button')
  ranking_page$clickElement()
  
  for (i in list_ivy) { #for all the ivys
      for (year in c(2003, 2013, 2023)) { #for all the years
        # select the scrollable element of year at the top of the page
        date_selector <- driver$findElement(using = 'class name', value = 'inputWrapper')
        Sys.sleep(1)
        date_selector$clickElement()
        Sys.sleep(1)
        
        # select year of interest in sequence, using modular xpath (2024-year of interest)
        date_element_xpath <- paste0('//*[@id="bar-content"]/div[1]/div/div[2]/ul/li[', (2024 - year), ']')
        date_element <- driver$findElement(using = "xpath", value = date_element_xpath)
        
        date_element$clickElement()
        Sys.sleep(1)
        
        # Select the search bar for universities, clear it and type each university one by one
        search_bar <- driver$findElement(using = "class", value = "search-input")
        search_bar$clearElement()
        search_bar$sendKeysToElement(list(i))
        Sys.sleep(1)
        search_bar$sendKeysToElement(list(key = "enter"))
        Sys.sleep(2)
  
        # Scrape the ranking
        select_ranking <- driver$findElement(using = 'xpath', value = '//*[@id="content-box"]/div[2]/table/tbody/tr/td[1]/div')
        
        ranking <- as.character(select_ranking$getElementText())
  
        # Store the result in Ivy_rankings
        Ivy_rankings <- do.call(rbind, list(Ivy_rankings, 
                                            data.frame(University = i, 
                                                       Year = year, 
                                                       Ranking = ranking)))    
        }
  }
  # closing the driver
  driver$close()
  rD$server$stop()
  return(Ivy_rankings)
}
```

After having called the function, which also saves the results in a data frame, I proceed on to handling formatting. Some rankings are provided as a range, for which I take the midpoint. I round to 0 decimal places as a ranking of eg. 12.5 is not substantively meaningful when comparing a university to another and is unnecessarily verbose.

```{r}
# Call the function to get Ivy League rankings
Ivy_rankings <- scrapeIvyRankings()
```
```{r}
# clean the results before outputting the final table
# to turn the rankings to numbers and find midpoint for range measures
# I round the ranking to 0 decimal places, since half points in ranking are not substantively significant

ARWU_ivy_ranking <- Ivy_rankings %>%
  mutate(
    Ranking = ifelse(grepl("-", Ranking), gsub("-", " ", Ranking), Ranking),
    Ranking = sapply(strsplit(Ranking, " "), function(x) if(length(x) > 1) sum(as.numeric(x)) / 2 else as.numeric(x)),
    Ranking = round(Ranking,0)
  )
# writing table to database
dbWriteTable(assignment2_db, "ARWU_ivy_ranking", ARWU_ivy_ranking)
```

I write the table to my database as ARWU_ivy_ranking and use my check_table function to verify its existence. The output here shows that it is successful. 
```{r, eval = TRUE, echo = TRUE}
# Checking that the table exists 
check_table(assignment2_db, "ARWU_ivy_ranking")
```
### Part B

For this section, I create an automated web-scraper which navigates to each university's main page and selects the social science option from the scroll menu. For this doing, I use the option of the last child, for some universities don't offer all study categories, and the x-path changes on individual pages. 

I scrape the entire table and deal with differing lengths between universities (ie. if  university does not offer Economics, for instance) by introducing NA's in those cases (rbind command). In this case, missing information is not with regards to a missing ranking, but rather a missing discipline at a university, which means that nothing can be done to remedy a missing value. I thus leave the missing values as they are, and will consider this when proceeding on to their analysis, where it might be best to consider universities that all share the same subjects, if our aim is to compare such characteristic. 

I also further clean the data post-scraping, and deal with range-rankings in the same process outlined above. 

```{r}
social_science_rank <- function() {
  # Input your list of universities here
  list_ivy <- c("Harvard University", "Princeton University", "Yale University", 
                "Columbia University", "University of Pennsylvania", "Brown University", 
                "Dartmouth College", "Cornell University")

  # Initialize the result table
  result_table <- data.frame()

  # Start the RSelenium driver
  rD <- rsDriver(browser = c("firefox"), verbose = FALSE, port = netstat::free_port(random = TRUE), chromever = NULL)
  driver <- rD[["client"]]

  # Navigate to the selected URL address
  driver$navigate("https://www.shanghairanking.com/")

  for (i in list_ivy) {
    # Navigate to the university page
    university_page <- driver$findElement(using = "xpath", value = '//*[@id="__layout"]/div/div[1]/div[1]/div/div[2]/ul/li[3]/a')
    university_page$clickElement()
    Sys.sleep(0.5)

    # Search for the search bar, clear it and type the university name
    large_search_bar <- driver$findElement(using = "class", value = 'input')
    large_search_bar$clickElement()
    Sys.sleep(1)
    
    large_search_bar$clearElement()
    large_search_bar$sendKeysToElement(list(i))
    Sys.sleep(1)
    
    large_search_bar$sendKeysToElement(list(key = "enter"))
    Sys.sleep(1)

    # Navigate to the social sciences page
    select_uni_page <- driver$findElement(using = "class", value = 'univ-main')
    select_uni_page$clickElement()
    Sys.sleep(1)

    social_science_select <- driver$findElement(using = "class", value = "inputWrapper")
    social_science_select$clickElement()
    Sys.sleep(0.5)

    social_science_click <- driver$findElement(using = "xpath", value = '//*[@id="gras"]/div[2]/div[1]/div[1]/div[2]/div/div[2]/ul/li[last()]')
    social_science_click$clickElement()
    Sys.sleep(0.5)

    # Extract rankings table
    social_sciences_rankings <- driver$findElement(using = 'class name', value = "table-container")
    rankings_html <- read_html(social_sciences_rankings$getElementAttribute('innerHTML')[[1]])


    # Transform the HTML table into a data frame
    rankings_table <- html_table(rankings_html)[[1]]

    # Add University column
    rankings_table$University <- i
 
    # Append to the result table
    result_table <- rbind(result_table, rankings_table)
  }

  # Return the result table and close driver
  driver$close()
  rD$server$stop()
  return(result_table)
}

# Call to function 
social_rank_table <- social_science_rank()
```

```{r, cleaning dataframe}
# If there's a range provided I take the mean ranking
# I round to 0 decimal places because "12.5" ranking is not substantively meaningful compared to 13 or 12
  # Especially since the main interest of ranking is to compare universities with each other 

clean_social_science <- social_rank_table %>%
  mutate(
    Rank = ifelse(grepl("-", Rank), gsub("-", " ", Rank), Rank),
    Rank = sapply(strsplit(Rank, " "), function(x) if(length(x) > 1) sum(as.numeric(x)) / 2 else as.numeric(x)),
    Rank = round(Rank, 0)
  )
dbWriteTable(assignment2_db, "Social_Science_Ivy_Ranking", clean_social_science)
```

I here write the table to my database as Social_science_Ivy_Ranking and check its existence. The output indicates this was successful. 

```{r, eval = TRUE, echo = TRUE}
#writing table to database
check_table(assignment2_db, "Social_science_Ivy_Ranking")
```
## Exercise 4

### Part A 

The API documentation provides us with information on how to use the EINs to gather information from its data. I thus use the the EIN list from my first table: R1R2_uni_list to select ivy university EINs. I then loop through each of these, representing an individual university, by using the organization method and the EIN. I extract the filing information (revenue and assets of interest) within filings_with_data. Different universities have differing lengths for these variables, with some years being absent for some, so each missing point is simply not included in the table. 

Due to the nature of the data (nested lists), in order to save into a table inside the function, I un-list it and bind it into a data frame. I make all further modifications outside the functions, so as to maximize the runtime speed of the API query. 

```{r}
get_ivy_info <- function() {
  # Create an empty list to store individual data frames
  results_list <- list()

  # Assuming merged_uni_table is your dataset
  # Replace "IsIvy" with the actual column name in your dataset
  eins_list <- merged_uni_table %>%
    filter(IsIvy == "Yes") %>%
    pull("EIN_ivys")  # Replace "EIN_ivys" with the actual column name in your dataset

  # Loop through Ivy League universities and fetch information
  for (ein in eins_list) {
    api_url <- paste0('https://projects.propublica.org/nonprofits/api/v2/organizations/', ein, '.json')

    # Make the GET request
    response <- GET(api_url)

    # Check if the request was successful (status code 200)
    if (status_code(response) != 200) {
      cat("Error:", status_code(response), "\n")
      cat("--------------------------------------------------\n")
      next()  # Skip to the next iteration if there's an error
    }

    data <- content(response, "parsed")

    # Extract relevant data
    ein_data <- data.frame(
      ein = data$organization$ein,
      year = sapply(data$filings_with_data, function(x) x$tax_prd_yr),
      revenue = sapply(data$filings_with_data, function(x) x$totrevenue),
      assets = sapply(data$filings_with_data, function(x) x$totassetsend),
      stringsAsFactors = FALSE
    )

    results_list <- bind_rows(results_list, ein_data)
  }

  return(results_list)
}
ivy_res <- get_ivy_info()
```

I proceed by merging the name column from my R1R2_uni_list table into the table retrieved from the API, so that the names of the universities will be the same across the tables, as the data from the API retrieves them differently. This will facilitate joining tables at a later stage. I then write it to my database as ivy_fiscal_info, and confirm that the process was successful. 

```{r}
# Cleaning the API output to be just the columns of interest 
ivy_res_cleaned <- ivy_res %>%
  mutate(ein = as.character(ein)) %>% #make sure variables are the same type to make merging easier 
  left_join(
    merged_uni_table %>%
      mutate(EIN_ivys = as.character(EIN_ivys)) %>%
      select(EIN_ivys, Name),
    by = c("ein" = "EIN_ivys")
  ) %>%
  select(Name, ein, year, revenue, assets)


# writing table to database
dbWriteTable(assignment2_db, "ivy_fiscal_info", ivy_res_cleaned)
```
```{r, eval = TRUE, echo = TRUE}
check_table(assignment2_db, "ivy_fiscal_info")
```

### Part B

For the packaged API, I choose the variable B19013_001 as it stands for the median income of a household in the last 12 months, adjusted for inflation. This is the only variable in the packaged API regarding household income that was not pre-weighted in some way (for gender, race, household size etc.). Furthermore, adjusted for inflation makes comparison between years more straightforward. 

I repeat the process for both 2015 and 2020, using the default year variable (5 years), since data is not available for all counties in the single year specification, and save the results to separate data frames; this facilitates the cleaning and merging process. 

```{r}
# Read the key from my envs folder
readRenviron("tidycensusapi.env")
census_api_key <- Sys.getenv("tidycensus_key")
```

```{r message = FALSE}
# choosing the county geography and the year 2015
# variable B19013_001 for the median household income adjusted for inflation 
housing_income2015 <- get_acs(geography = "county", 
              variables = c(medincome = "B19013_001"), 
              year = 2015)
housing_income2020 <- get_acs(geography = "county", 
              variables = c(medincome = "B19013_001"), 
              year = 2020)
```

I merge the two data frames. For this doing, I use a regex to extract the first part of the county_state information in the R1R2_uni_list table to obtain a partial match in the API data. In a case where there would be an undesired match (eg. Mercer County, NJ and Mercer County ND), I select the first occurrence. In this case, New Jersey is earlier in the alphabet than North Dakota and this is the only case I seek to prevent here. This is due to the fact that the variable for county in the packaged API contains shortened state names, while the R1R2_uni_list contains full names of both state and county. I keep only the name of the university, its county and the 2015 estimate. 

I repeat the process for 2020 as well. This constitutes the final table, where there is no missing data. I write the table to my database as ivy_county_income, and confirm its successful upload. 


```{r}
# I take ivy universities and merge them with the 2015 housing income table. For this, I have to match the first part of the county
housing_info2015 <- merged_uni_table %>%
  filter(IsIvy == "Yes") %>%
  mutate(county_state_match = str_extract(county_state, "^[^,]+, [A-Z]")) %>%
  left_join(housing_income2015 %>%
              select(NAME, estimate) %>%
              mutate(county_state_match = str_extract(NAME, "^[^,]+, [A-Z]")),
            by = "county_state_match") %>%
  distinct(county_state_match, .keep_all = TRUE) %>%
  select(Name, county_state, estimate) %>%
  rename("2015" = estimate) %>%
  mutate(county_state = str_replace(county_state, "\n", ""))


# Repeat the process but adding 2020 as well
# Then we can pivot it longer
housing_info2020 <- housing_info2015 %>%
  mutate(county_state_match = str_extract(county_state, "^[^,]+, [A-Z]")) %>%
  left_join(housing_income2020 %>%
              select(NAME, estimate) %>%
              mutate(county_state_match = str_extract(NAME, "^[^,]+, [A-Z]")), 
            by = "county_state_match") %>%
  distinct(county_state_match, .keep_all = TRUE) %>%
  rename("2020" = estimate) %>%
  select(Name, county_state, "2015", "2020")

housing_info_long <- housing_info2020 %>%
  pivot_longer(cols = c("2015", "2020"), names_to = "year", values_to = "estimate")
```

```{r}
dbWriteTable(assignment2_db, "ivy_county_income", housing_info_long)
```
```{r, eval = TRUE, echo = TRUE}
check_table(assignment2_db, "ivy_county_income")
```

## Exercise 5 

### Part A

In order to obtain for all the Ivys only, information about the endowment per student, average overall and social science ranking, county income and revenue per student, I write a large SQL query into my database joining all the tables with "LEFT JOIN" on the Name (or University) variable. I use grouping and filtering conditions to select the Ivys and only the variables of interest, and perform some calculations directly in the queries. 

For instance, for the total endowment, the variable written to my database contains "billion" and "million" as well as a dollar sign, which prevents direct calculations. I thus use REPLACE and LIKE to convert these into numbers that can be processed. Given that I already matched the county names in the previous stage, I do not need to repeat this here, and can take the variables as they are to join. 

As mentioned earlier, the left joins are much simpler with the variable "Name" being the same for the API data, as it can now just be joined on the same key with the other tables, which is helpful in large query like this one. 

```{r eval = TRUE}
# Connecting to the database 
db <- dbConnect(RSQLite::SQLite(), "assignment2_database.db")
```

```{r eval = TRUE}
insights_table <- dbGetQuery(db, "WITH EndowmentCTE AS (
    SELECT
        R1R2_uni_list.Name AS University,
        (CASE
            WHEN R1R2_uni_list.Endowment LIKE '%$%' AND R1R2_uni_list.Endowment LIKE '%billion%' THEN
                CAST(REPLACE(REPLACE(R1R2_uni_list.Endowment, '$', ''), ' billion', '') AS DECIMAL(20, 2)) * 1000000000
            WHEN R1R2_uni_list.Endowment LIKE '%$%' AND R1R2_uni_list.Endowment LIKE '%million%' THEN
                CAST(REPLACE(REPLACE(R1R2_uni_list.Endowment, '$', ''), ' million', '') AS DECIMAL(20, 2)) * 1000000
            ELSE
                CAST(REPLACE(R1R2_uni_list.Endowment, '$', '') AS DECIMAL(20, 2))
        END / R1R2_uni_list.TotalStudents) AS EndowmentPerStudent,
        a.AverageOverallRanking,
        s.SocialScienceRank
    FROM R1R2_uni_list
    LEFT JOIN ARWU_Ivy_ranking ivy ON R1R2_uni_list.Name = ivy.University
    LEFT JOIN (
        SELECT
            s1.University,
            ROUND(AVG(r.Ranking), 2) AS AverageOverallRanking
        FROM Social_Science_Ivy_Ranking s1
        LEFT JOIN ARWU_ivy_ranking r ON s1.University = r.University
        GROUP BY s1.University
    ) a ON R1R2_uni_list.Name = a.University
    LEFT JOIN (
        SELECT
            s2.University,
            ROUND(AVG(s2.Rank), 2) AS SocialScienceRank
        FROM Social_Science_Ivy_Ranking s2
        WHERE s2.Subject IN ('Economics', 'Political Sciences', 'Sociology')
        GROUP BY s2.University
    ) s ON R1R2_uni_list.Name = s.University
    WHERE ivy.University IS NOT NULL
    GROUP BY s.University
),

CountyIncomeCTE AS (
    SELECT
        Name AS University,
        county_state,
        ROUND(AVG(estimate), 2) AS AverageCountyIncome
    FROM ivy_county_income
    GROUP BY Name, county_state
),

RevenuePerStudentCTE AS (
    SELECT
        i.Name,
        ROUND(AVG(i.revenue / COALESCE(r.TotalStudents, 1)), 2) AS AverageRevenuePerStudent
    FROM ivy_fiscal_info i
    LEFT JOIN R1R2_uni_list r ON i.Name = r.Name
    GROUP BY i.Name
)

SELECT
    e.University,
    c.county_state AS County,
    e.EndowmentPerStudent,
    e.AverageOverallRanking,
    e.SocialScienceRank,
    c.AverageCountyIncome,
    r.AverageRevenuePerStudent
FROM EndowmentCTE e
LEFT JOIN CountyIncomeCTE c ON e.University = c.University
LEFT JOIN RevenuePerStudentCTE r ON e.University = r.Name;
")
```
#### Plotting Ivy League Variables 

As it can be seen in the below Figure 1, it appears that there is a positive relationship between the average university ranking across 2003, 2013 and 2023 and the average social science ranking in 2023 for ivy league universities. The higher the overall ranking value (lower ranked) the higher the social science ranking value (lower rank). Intuitively, it makes sense that the lower ranked the university overall, the lower the social science ranking will be, as social science is computed as part of the mean ranking. 

Some universities show disproportionately high values (low rankings) compared to others, which are not explainable without further information on the specific subjects offered by that university. It is possible, for instance, that the university offers a disproportionately large number of subjects, in which is it not highly competitive in the world rankings, and that it excels in some disciplines. 

Nonetheless, the trend visible in the data reflects the key intuition that a well-ranked university overall will have well-ranked, on average, social sciences courses. Of course, the reverse may also be true in that an excellent social science performance will inevitably contribute to a higher overall rank, but this is not verifiable given we are only considering a small subset of social science subjects, and we are not considering this in the context of their proportion compared to the wider number of subjects. 

```{r eval = TRUE, message=FALSE}
# Plot 1: Average university ranking vs. average Econ/PS/Soc ranking
ggplot(insights_table, aes(x = AverageOverallRanking, y = SocialScienceRank)) +
  geom_point(size = 4, color = "hotpink") +
  labs(title = "Figure 1: Average University Ranking vs. Average Econ/PS/Soc Ranking",
       x = "Average University Ranking",
       y = "Average Econ/PS/Soc Ranking") +
  geom_smooth(method='lm', col = "blue", linetype = "dotted", fill = "lightpink", alpha = 0.2) + #adding regression line 
  annotate("text", x = 200, y = 65, label = "Regression Line", color = "black", size = 3) +  #labelling regression line 
  theme_minimal()
```

As Figure 2 below demonstrates, there is a general trend between endowment per student and average university ranking which indicates that the lower the ranking value (the higher ranked), the higher the endowment per student value. This is intuitive as we might assume that better ranked universities have larger endowment amounts to devote to their educational facilities. 

I note that this is only representative of ivy universities, which are considered to be well-endowed and well-ranked overall, and these changes are relative. They may not apply to other universities, or university types. However, in this context we may plausibly assume that a better ranking means higher endowment as the university suscitates interest and donations, though the reverse may also be true in that a higher endowment leads to better academic resources and thus higher performance and ultimately ranking.

```{r eval = TRUE}
# Plot 2: Average university ranking vs. endowment per student
ggplot(insights_table, aes(x =AverageOverallRanking , y = EndowmentPerStudent)) +
  geom_point(size = 5, color = "hotpink") +
  labs(title = "Figure 2: Average University Ranking vs. Endowment Per Student (USD) ",
       x = "Average University Ranking ",
       y = "Endowment Per Student (USD)")+
  geom_smooth(method='lm', se = FALSE, col = "blue", linetype = "dotted") +
  scale_y_continuous(labels = scales::comma)+
  annotate("text", x = 200, y =  400000, label = "Regression Line", color = "black", size = 3) + 
  theme_minimal()
```

Figure 3 below shows the relationship between the endowment per student on average and the average median household income in the county where the university is found. There is no exceedingly apparent trend between the variables, but it appears that high endowment universities are generally found in high household income counties. While this is not surprising, there is a clear outlier that stands out: Columbia university. Situated in New York City, one would expect the median county income to on average be high, but the endowment is comparatively low, which is surprising. On the reverse, Princeton university has the highest endowment per student of all, though the county in which it is situated is a non-urban part of the country, which justifies a lower median county household income on average. 

Overall, there is no clear pattern between these two variables, and the intuition is not so clear in this case. Endowment is a measure of the university's financial "popularity", which in the case of ivy universities tends to widely depend on donations, rather than as a measure of the its county's wealth in terms of income. One might have expected the university to require (and thus seek to obtain) high endowments if situated in high income counties, which in many cases are also more expensive to live in, but this is not particularly obvious or capturable through these variables. 

```{r eval = TRUE}
# Plot 3: Endowment per student vs. average median household income
lab_pos <- data.frame(University =c("Harvard University", "Princeton University", "Yale University", 
                "Columbia University", "University of Pennsylvania", "Brown University", 
                "Dartmouth College", "Cornell University"), 
                x = c(2300000, 3800000, 3400000, 620000, 800000, 800000, 1300000, 500000), 
                y = c(92000, 83000, 72000, 84000, 47000, 53000, 65000, 59000))
ggplot(insights_table, aes(x = EndowmentPerStudent, y = AverageCountyIncome, size = EndowmentPerStudent)) +
  geom_point(aes(color = University), alpha = 0.7, show.legend = FALSE) +
  scale_size_continuous(range = c(3, 12)) +  #dot size range
  labs(title = "Figure 3: Endowment Per Student vs. Average Median Household Income",
       x = "Endowment Per Student (USD)",
       y = "Average Median Household Income (USD - Adjusted for Inflation)",
       caption = "Bubble size = Endowment Per Student") +
  scale_x_continuous(labels = scales::comma)+
  scale_y_continuous(labels = scales::comma)+ 
  geom_text(data = lab_pos, aes(x = x, y = y, label = University), size = 3)+
  theme_minimal() +
  theme(legend.position = "bottom", legend.title = element_blank())
```
In Figure 4 below, we can see clearly that the average revenue per student is systematically higher than the average median county household income. There doesn't seem to be an upward pattern where a higher county median income associates with a higher revenue per student. I note here that this is not necessarily surprising given that universities generate revenue from endowment, tuition fees, research and other sources of private funding, and that the measures compared here are not necessarily weighted in the same way. The average revenue of universities per student is a measure per tax year, averaged over the last ten years, but the median income was adjusted for inflation, and as it is the median it is naturally more resistant to fluctuations. We thus ought to keep this in mind when comparing these two measures. 

Furthermore, as I mentioned earlier, many Ivys are situated in rural areas, or at the very least in smaller cities, which would also justify a lower household income to a certain extent as cost of living is generally lower and employment opportunities are fewer. 

```{r eval = TRUE, message = FALSE, warning=FALSE}
# Plot 4: Average revenue per student vs. average median household income
# put data into long format so that it's easier to make levels for the data
insights_long <- insights_table %>%
  mutate(University = factor(University)) %>%
  pivot_longer(cols = c(AverageRevenuePerStudent, AverageCountyIncome),
               names_to = "Variable", values_to = "Values")

# grouped bar plot 
ggplot(data = insights_long, aes(x = University, y = Values, fill = Variable)) +
  geom_bar(stat = "identity", position = position_dodge(width = 0.9), width = 0.7) +
  labs(title = "Figure 4: Average Revenue per Student vs. Average Median Household Income",
       x = NULL, y = "USD") +
  scale_y_continuous(labels = scales::comma, limits = c(0, 550000)) +
  scale_x_discrete(labels = NULL) +
  theme_minimal() +
  facet_grid(. ~ Variable) +
  geom_text(aes(label = University),
            position = position_dodge(width = 0.9),
            size = 3,
            vjust = 0.5,
            angle = 90,
            hjust = -0.3)+
  guides(fill = FALSE)
```

As Figure 5 below illustrates, there is a linear trend visible between average ranking and average revenue, where universities with higher revenues are usually better ranked, with the exception of Dartmouth. It is intuitive that these two variables are to some extent cyclical: higher revenue drives better investment in teaching resources, which increases ranking, which increases student applications and intake, which in turn contributes to revenue. To say that one causes the other would be a stretch in this situation, but it is clear that there is a link, apart from the outlier university. 

The size of the bubbles in this graph represents the ranking, where larger bubbles signify better ranks, and all the largest bubbles are situated where revenue per student is highest, which is at the very least an indication that there is positive relationship between the two variables, though further inference is not possible. 

```{r eval = TRUE, message = FALSE}
# Plot 5: Average revenue per student vs. average university ranking
lab_pos1 <- data.frame(University = c("Harvard University", "Princeton University", "Yale University", 
                "Columbia University", "University of Pennsylvania", "Brown University", 
                "Dartmouth College", "Cornell University"), 
                x = c(315000, 230000, 365000, 135000, 280000, 140000, 190000, 190000), 
                y = c(0, 7, 21, 25, 30, 50, 210, 26))

ggplot(insights_table, aes(x = AverageRevenuePerStudent, y = AverageOverallRanking, size = AverageOverallRanking, fill = University)) +
  geom_point(aes(color = University), alpha = 0.7, show.legend = FALSE) +
  labs(title = "Figure 5: Average Revenue Per Student vs. Average University Ranking",
       x = "Average Revenue Per Student (USD)",
       y = "Average University Ranking", 
       caption = "Bubble size = Ranking (large bubble = low ranking)") +
  scale_x_continuous(labels = scales::comma) +
  geom_text(data = lab_pos1, aes(x = x, y = y, label = University), size = 3)+
  scale_size_continuous(range = c(10, 3)) +  # Adjust the range for bubble sizes
  theme(legend.position = "none")+
  theme_minimal()+ theme(legend.position = "bottom", legend.title = element_blank())
```

### Part B

For the second part of this exercise, from my database, I retrieve the variables I will need for the map (Name, Coordinates, Status, Ivy). I then separate the latitude from the longitude using a regex, and then converting them into decimals from Days, Minutes, Seconds format. I use a flexible function that will work for elements that do not have seconds. I also ensure here that the longitude (West in this case) is negative, so that it will plot the points onto the US map and not on the other side of the world at a mirrored longitude, so I multiply the numbers by negative one.

Second, before plotting the map, I convert my IsIvy variable into factors, to ensure that the correct levels are captured in the map. I then convert my table into a spatial data frame and specify crs = 4326 for the WGS 84 coordinate system, to ensure that every element is ready to be plotted. 

Finally, I create a US map layer, and an additional two layers for the status and the ivy status of universities before plotting an interactive map using tmap. 

```{r eval = TRUE, message = FALSE}
# Connect to database and use simple SQL query to get the information 
db<-dbConnect(RSQLite::SQLite(), "assignment2_database.db")
university_data <- dbGetQuery(db, "SELECT
           Name, 
           Coordinates, 
           Status, 
           IsIvy
        FROM R1R2_uni_list
        ")
options(tigris_use_cache = TRUE) #follow the documentation on how to retrieve data from package
```
```{r eval = TRUE, message = FALSE}
us_map <- tigris::states(class = "sf") # retrieve shapefile from data

# separate the latitude and longitude 
processed_data <- university_data %>%
  mutate(
    Coordinates = sapply(strsplit(Coordinates, "\\s+"), function(x) paste(x[1], x[2], sep = ",")), 
    Latitude = sapply(strsplit(Coordinates, ","), `[`, 1),
    Longitude = sapply(strsplit(Coordinates, ","), `[`, 2)
  ) %>%
  select(-Coordinates)

# function which transforms DMS coordinates into decimals 
dms_to_decimal <- function(coord) {
  # Extract degrees, minutes, seconds, and direction using regex
  parts <- strsplit(coord, "[^0-9.]+")[[1]]

  # Convert parts to numeric values
  degrees <- as.numeric(parts[1])
  minutes <- ifelse(length(parts) >= 3, as.numeric(parts[2]), 0)
  seconds <- ifelse(length(parts) >= 4, as.numeric(parts[3]), 0)
  
  # calculate decimal degrees
  decimal <- degrees + minutes / 60 + seconds / 3600

  return(decimal)
}

# apply to data
processed_data$Latitude <- sapply(processed_data$Latitude, dms_to_decimal)

# multiply by -1 to have the negative longitude (West)
processed_data$Longitude <- sapply(processed_data$Longitude, dms_to_decimal) * (-1)
```

```{r eval = TRUE, message = FALSE}
# Filter out rows with missing coordinates
coordinates_data <- processed_data[complete.cases(processed_data[, c("Longitude", "Latitude")]), ]

# Convert IsIvy to a factor with levels rather than a character object
coordinates_data$IsIvy <- factor(coordinates_data$IsIvy, levels = c("No", "Yes"))

# Convert coordinates_data to sf for compatibility 
sf_coordinates_data <- st_as_sf(coordinates_data, coords = c("Longitude", "Latitude"), crs = 4326)

# create a first layer for the public and private universities
status_layer <- tm_shape(sf_coordinates_data %>% filter(IsIvy == "No")) + 
  tm_dots(
    size = 0.1, 
    col = "Status", 
    palette = c("royalblue", "lightgreen"), 
    legend.show =  FALSE
  )
# create a second layer for the ivys
ivy_layer <- tm_shape(sf_coordinates_data %>% filter(IsIvy == "Yes")) + 
  tm_dots(size = 0.1, 
          col = "IsIvy", 
          palette = c("red"),
          legend.show = FALSE)

# get map
tm_us <- tm_shape(us_map) + 
  tm_borders(alpha = 0.03)

# create the legend
legend <- tm_add_legend(type = "fill", col = c("lightgreen", "royalblue", "red"), 
                        labels = c("Public", "Private (non-profit)", "Ivy"), 
                        title = "University Type")

# create interactive map 
map <- tm_us + status_layer + ivy_layer + 
  tm_layout(title = "Universities in the United States", legend.position = c("left", "bottom")) +
  legend

# Display the map
tmap_mode("view")
map
```

The map above illustrates the location of different categories of universities in the US. Ivy league universities are essentially all in the North East of the country, mostly along the coast. Conversely, private and public universities are more spread out, though private universities tend to be on the coasts, while public ones are more generally in central states. Public universities are also much more compared to private ones and Ivys, which is in line with intuitions about the education system in the US; private universities are widely considered better and are thus fewer in number. 

Furthermore, it is clear from the map that some areas of the country have very few universities, particularly on the central-West to West coast. Aside from a "hub" of universities in California, there are very few compared to the East coast. It is also evident that areas with few universities have mostly public universities. 

A first explanation for this phenomenon would be the sparse population in the US in most mid-Western states, with the country's main metropolitan cities being on the coasts, or around the gulf. This explains a lesser need for universities in that area, and the higher probability of them being public; private universities may also be more likely to attract international students as well, for which big cities are better equipped.

Furthermore, parser population also means fewer job opportunities. This makes universities difficult to establish and justify in terms of costs. It is also difficult to establish a new university, as they largely go by reputation, in a community/state where there is little opportunity for social and educational mobility. More rural areas might also not place such a high value on higher education as a metropolitan center would, which contributes to the lack of incentive to establish a university in rural spaces. 

Finally, it is also possible that the Ivys being virtually the first universities in the country, that other universities developed in waves around them. This would explain the hub on the East coast, close to the Ivys and the progressive spread West, with the exception of California which is one of the most populated states in the country, therefore requiring more universities. 

## Appendix Code
```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE} 
# this chunk generates the complete code appendix. 
# eval=FALSE tells R not to run (``evaluate'') the code here (it was already run before).
```